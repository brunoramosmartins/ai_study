{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Please complete the `compute_cost` function using the equations below.\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
    "    * It might be helpful to first calculate an intermediate variable $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ where $n$ is the number of features, before calculating $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$\n",
    "\n",
    "Note:\n",
    "* As you are doing this, remember that the variables `X_train` and `y_train` are not scalar values but matrices of shape ($m, n$) and ($ùëö$,1) respectively, where  $ùëõ$ is the number of features and $ùëö$ is the number of training examples.\n",
    "* You can use the sigmoid function that you implemented above for this part.\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 2\n",
    "\n",
    "Complete a fun√ß√£o `compute_cost` usando as equa√ß√µes abaixo.\n",
    "\n",
    "Lembre-se de que, para a regress√£o log√≠stica, a fun√ß√£o de custo tem a forma\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}( \\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "onde\n",
    "* m √© o n√∫mero de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ √© o custo de um √∫nico ponto de dados, que √© -\n",
    "\n",
    "     $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left (f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ √© a previs√£o do modelo, enquanto $y^{(i)}$, que √© o r√≥tulo real\n",
    "\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ onde a fun√ß√£o $g$ √© a fun√ß√£o sigmoide.\n",
    "     * Pode ser √∫til primeiro calcular uma vari√°vel intermedi√°ria $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{( i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ onde $n$ √© o n√∫mero de fei√ß√µes, antes de calcular $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{( e)}))$\n",
    "\n",
    "Observa√ß√£o:\n",
    "* Ao fazer isso, lembre-se de que as vari√°veis `X_train` e `y_train` n√£o s√£o valores escalares, mas matrizes de forma ($m, n$) e ($ùëö$,1) respectivamente, onde $ùëõ$ √© o n√∫mero de recursos e $ùëö$ √© o n√∫mero de exemplos de treinamento.\n",
    "* Voc√™ pode usar a fun√ß√£o sigmoide implementada acima para esta parte.\n",
    "\n",
    "Se voc√™ tiver d√∫vidas, pode conferir as dicas apresentadas ap√≥s a c√©lula abaixo para ajud√°-lo na implementa√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Please complete the `compute_gradient` function to compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label\n",
    "\n",
    "\n",
    "- **Note**: While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $f_{\\mathbf{w},b}(x)$.\n",
    "\n",
    "As before, you can use the sigmoid function that you implemented above and if you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 3\n",
    "\n",
    "Complete a fun√ß√£o `compute_gradient` para calcular $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)} {\\partial b}$ das equa√ß√µes (2) e (3) abaixo.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf {w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf {w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m √© o n√∫mero de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "    \n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ √© a previs√£o do modelo, enquanto $y^{(i)}$ √© o r√≥tulo real\n",
    "\n",
    "\n",
    "- **Observa√ß√£o**: embora esse gradiente pare√ßa id√™ntico ao gradiente de regress√£o linear, a f√≥rmula √© realmente diferente porque a regress√£o linear e a log√≠stica t√™m diferentes defini√ß√µes de $f_{\\mathbf{w},b}(x)$.\n",
    "\n",
    "Como antes, voc√™ pode usar a fun√ß√£o sigmoide que voc√™ implementou acima e, se tiver d√∫vidas, pode conferir as dicas apresentadas ap√≥s a c√©lula abaixo para ajud√°-lo na implementa√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <details>\n",
    "   <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "* Veja como voc√™ pode estruturar a implementa√ß√£o geral dessa fun√ß√£o\n",
    "     ```python\n",
    "        def compute_gradient(X, y, w, b, *argv):\n",
    "             m, n = X. forma\n",
    "             dj_dw = np.zeros(w.forma)\n",
    "             dj_db = 0.\n",
    "        \n",
    "             ### COMECE O C√ìDIGO AQUI ###\n",
    "             para i no intervalo(m):\n",
    "                 # Calcule f_wb (exatamente como voc√™ fez na fun√ß√£o compute_cost acima)\n",
    "                 f_wb =\n",
    "        \n",
    "                 # Calcule o gradiente para b a partir deste exemplo\n",
    "                 dj_db_i = # Seu c√≥digo aqui para calcular o erro\n",
    "        \n",
    "                 # adicione isso a dj_db\n",
    "                 dj_db += dj_db_i\n",
    "        \n",
    "                 # obt√©m dj_dw para cada atributo\n",
    "                 para j no intervalo(n):\n",
    "                     # Voc√™ codifica aqui para calcular o gradiente do i-√©simo exemplo para o j-√©simo atributo\n",
    "                     dj_dw_ij =\n",
    "                     dj_dw[j] += dj_dw_ij\n",
    "        \n",
    "             # divide dj_db e dj_dw pelo n√∫mero total de exemplos\n",
    "             dj_dw = dj_dw / m\n",
    "             dj_db = dj_db/m\n",
    "             ### C√ìDIGO FINAL AQUI ###\n",
    "       \n",
    "             retornar dj_db, dj_dw\n",
    "     ```\n",
    "  \n",
    "     Se voc√™ ainda est√° preso, pode verificar as dicas apresentadas abaixo para descobrir como calcular `f_wb`, `dj_db_i` e `dj_dw_ij`\n",
    "    \n",
    "      <details>\n",
    "           <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular f_wb</b></font></summary>\n",
    "            &emsp; &emsp; Lembre-se de que voc√™ calculou f_wb em <code>compute_cost</code> acima ‚Äî para dicas detalhadas sobre como calcular cada termo intermedi√°rio, confira a se√ß√£o de dicas abaixo desse exerc√≠cio\n",
    "             <details>\n",
    "               <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas para calcular f_wb</b></font></summary>\n",
    "               &emsp; &emsp; Voc√™ pode calcular f_wb como\n",
    "                <pre>\n",
    "                para i no intervalo(m):\n",
    "                    # Calcule f_wb (exatamente como voc√™ fez na fun√ß√£o compute_cost acima)\n",
    "                    z_wb = 0\n",
    "                    # Fa√ßa um loop sobre cada recurso\n",
    "                    para j no intervalo (n):\n",
    "                        # Adicione o termo correspondente a z_wb\n",
    "                        z_wb_ij = X[i, j] * w[j]\n",
    "                        z_wb += z_wb_ij\n",
    "            \n",
    "                    # Adicionar termo de vi√©s\n",
    "                    z_wb += b\n",
    "        \n",
    "                    # Calcula a previs√£o do modelo\n",
    "                    f_wb = sigmoide(z_wb)\n",
    "      </details>\n",
    "        \n",
    "      </details>\n",
    "      <details>\n",
    "           <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular dj_db_i</b></font></summary>\n",
    "            &emsp; &emsp; Voc√™ pode calcular dj_db_i como <code>dj_db_i = f_wb - y[i]</code>\n",
    "      </details>\n",
    "        \n",
    "      <details>\n",
    "           <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular dj_dw_ij</b></font></summary>\n",
    "         &emsp; &emsp; Voc√™ pode calcular dj_dw_ij como <code>dj_dw_ij = (f_wb - y[i])* X[i][j]</code>\n",
    "      </details>\n",
    "\n",
    " </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
